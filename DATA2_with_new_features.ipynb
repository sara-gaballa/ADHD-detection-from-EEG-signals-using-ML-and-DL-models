{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sara-gaballa/ADHD-detection-from-EEG-signals-using-ML-and-DL-models/blob/main/DATA2_with_new_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io2JFOXrKZZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef9b3b9e-a648-4589-a0aa-5ac9737784af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXLheiIAOqzs"
      },
      "source": [
        "##Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwEqoyeRMgmA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Define the feature extraction functions\n",
        "def mean(data):\n",
        "    return np.mean(data, axis=-1)\n",
        "\n",
        "def std(data):\n",
        "    return np.std(data, axis=-1)\n",
        "\n",
        "def minim(data):\n",
        "    return np.min(data, axis=-1)\n",
        "\n",
        "def maxim(data):\n",
        "    return np.max(data, axis=-1)\n",
        "\n",
        "def median(data):\n",
        "    return np.median(data, axis=-1)\n",
        "\n",
        "def q1(data):\n",
        "    return np.percentile(data, 25, axis=-1)\n",
        "\n",
        "def q3(data):\n",
        "    return np.percentile(data, 75, axis=-1)\n",
        "\n",
        "def skewness(data):\n",
        "    return stats.skew(data, axis=-1)\n",
        "\n",
        "def kurtosis(data):\n",
        "    return stats.kurtosis(data, axis=-1)\n",
        "\n",
        "def extract_features(data):\n",
        "    return np.concatenate((mean(data), std(data), minim(data), maxim(data),\n",
        "                           median(data), q1(data), q3(data), kurtosis(data), skewness(data)), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig1FFSvXOuOa"
      },
      "source": [
        "##DATA 2 importing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvMrZUhhOlEd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Dataset 2/data files'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KTVki7LOx1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9818a3f-9a40-432f-ab43-e32b96eecb0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mat73\n",
            "  Downloading mat73-0.62-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from mat73) (3.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mat73) (1.23.5)\n",
            "Installing collected packages: mat73\n",
            "Successfully installed mat73-0.62\n"
          ]
        }
      ],
      "source": [
        "pip install mat73"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OSW8_jNOx6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f077ff13-94b6-408d-ffb8-ddf0954b7ea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File d1.mat does not exist.\n",
            "File d2.mat does not exist.\n",
            "File d3.mat does not exist.\n",
            "File d4.mat does not exist.\n",
            "File d5.mat does not exist.\n",
            "File d6.mat does not exist.\n",
            "File d7.mat does not exist.\n"
          ]
        }
      ],
      "source": [
        "import scipy.io\n",
        "import os\n",
        "import numpy as np\n",
        "import mat73\n",
        "\n",
        "# Prefix and suffix of the filenames\n",
        "prefix = 'd'\n",
        "suffix = '.mat'\n",
        "# Range of file numbers to read\n",
        "start_num = 1\n",
        "end_num = 7\n",
        "\n",
        "# Initialize an empty list to store the extracted data\n",
        "data_list = []\n",
        "\n",
        "# Loop over the desired file numbers and read the corresponding .mat files\n",
        "for num in range(start_num, end_num+1):\n",
        "  # Construct the filename\n",
        "  filename = prefix + str(num) + suffix\n",
        "  filepath = os.path.join(folder_path, filename)\n",
        "\n",
        "  # Check if file exists\n",
        "  if os.path.exists(filepath):\n",
        "      # Load the .mat file and extract the desired variables\n",
        "      mat = mat73.loadmat(filepath)\n",
        "\n",
        "      # Append the extracted data to the list of data\n",
        "      data_list.append(np.array(mat[prefix+str(num)]))\n",
        "  else:\n",
        "      print('File ' + filename + ' does not exist.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "r3gxtqa9O1d-",
        "outputId": "f0660e15-0228-48ba-c1e4-79b499d4ca4b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c75210d5e8de>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Concatente all arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# Concatente all arrays\n",
        "data = data_list[0]\n",
        "for i in range(1, 7):\n",
        "  data = np.concatenate((data, data_list[i]))\n",
        "print(data.shape)\n",
        "del(data_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La8uvfObO2rg"
      },
      "outputs": [],
      "source": [
        "control_trials = 10129\n",
        "ADD_trials = 13031\n",
        "ADHD_trials = 10742\n",
        "\n",
        "HC = np.zeros(control_trials)\n",
        "ADHD = np.ones(ADD_trials + ADHD_trials)\n",
        "\n",
        "y = np.concatenate((HC, ADHD))\n",
        "print(y.shape);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UCLDcNnO4UK"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm_notebook\n",
        "features2=[]\n",
        "for trial in tqdm_notebook(data):\n",
        "    features2.append(extract_features(trial))\n",
        "features2=np.array(features2)\n",
        "features2.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhPeb-AwO6zo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(features2, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2z9phkxPdyV"
      },
      "source": [
        "##ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS0AaXbkPaTt"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.neural_network import MLPClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi_FjohgPc7V"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "models = { 'Logistic Regression': LogisticRegression(),\n",
        "           'Random Forest': RandomForestClassifier(),\n",
        "           'KNeighbors':KNeighborsClassifier(),\n",
        "            'Decision Tree':DecisionTreeClassifier(),\n",
        "            'Gradient Boosting':GradientBoostingClassifier(),\n",
        "            'SVC': SVC(),\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "params = { 'Logistic Regression': {'logisticregression__C': [0.1, 1, 10]},\n",
        "           'Random Forest': {'randomforest__n_estimators': [10, 100, 1000],\n",
        "                             'randomforest__max_depth': [None, 5, 10]},\n",
        "           'KNeighbors':{'kneighbors__n_neighbors': [3, 5, 10], 'kneighbors__metric': ['euclidean', 'manhattan', 'minkowski']},\n",
        "           'Decision Tree':{'decisiontree__max_depth': [None, 5, 10], 'decisiontree__criterion': ['gini', 'entropy']},\n",
        "           'Gradient Boosting':{'gradientboosting__learning_rate': [0.5, 1, 1.5], 'gradientboosting__max_depth': [10, 15]},\n",
        "            'SVC': {'svc__C': [0.1, 1, 10], 'svc__kernel': ['linear', 'rbf','poly']},\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "results = pd.DataFrame(columns=['Model', 'Parameters', 'Accuracy'])\n",
        "for name, model in models.items():\n",
        "  pipe = Pipeline([('scaler', StandardScaler()),(name.lower().replace(' ',''), model)])\n",
        "  grid = GridSearchCV(pipe, params[name], cv=5, scoring='accuracy')\n",
        "\n",
        "  # Fit the gridsearchcv on the train data\n",
        "  grid.fit(x_train, y_train)\n",
        "\n",
        "  # Predict on the test data\n",
        "  y_pred = grid.predict(x_test)\n",
        "\n",
        "  # Compute the accuracy score\n",
        "  acc = accuracy_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred)\n",
        "  recall = recall_score(y_test, y_pred)\n",
        "  f1 = f1_score(y_test, y_pred)\n",
        "  auroc = roc_auc_score(y_test, y_pred)\n",
        "  print(name, acc)\n",
        "  # Append the results to the dataframe\n",
        "  results = results.append({'Model': name, 'Parameters': grid.best_params_, 'Accuracy': acc,'Precision':precision,'Recall':recall,'F1':f1,'AURoc':auroc}, ignore_index=True)\n",
        "test_4=pd.DataFrame({'actual':y_test, 'pred':y_pred})\n",
        "results = results.sort_values(by='Accuracy', ascending=False)\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "vAknN3Oi_cPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "models = {\n",
        "           'MLPClassifier':MLPClassifier()}\n",
        "\n",
        "\n",
        "params = {\n",
        "           'MLPClassifier':{'mlpclassifier__solver':['lbfgs','adam','sigmoid'],'mlpclassifier__alpha':[0.01,0.001,0.0001,0.00001,1e-5],'mlpclassifier__hidden_layer_sizes':[(10, 10),(10,5),(10,2),(5,5),(5,2)],'mlpclassifier__random_state':[1,None]}}\n",
        "\n",
        "\n",
        "results = pd.DataFrame(columns=['Model', 'Parameters', 'Accuracy'])\n",
        "for name, model in models.items():\n",
        "  pipe = Pipeline([('scaler', StandardScaler()),(name.lower().replace(' ',''), model)])\n",
        "  grid = GridSearchCV(pipe, params[name], cv=5, scoring='accuracy')\n",
        "# Fit the gridsearchcv on the train data\n",
        "  grid.fit(x_train, y_train)\n",
        "\n",
        "  # Predict on the test data\n",
        "  y_pred = grid.predict(x_test)\n",
        "\n",
        "  # Compute the accuracy score\n",
        "  acc = accuracy_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred)\n",
        "  recall = recall_score(y_test, y_pred)\n",
        "  f1 = f1_score(y_test, y_pred)\n",
        "  auroc = roc_auc_score(y_test, y_pred)\n",
        "  print(name, acc)\n",
        "  # Append the results to the dataframe\n",
        "  results = results.append({'Model': name, 'Parameters': grid.best_params_, 'Accuracy': acc,'Precision':precision,'Recall':recall,'F1':f1,'AURoc':auroc}, ignore_index=True)\n",
        "test_4=pd.DataFrame({'actual':y_test, 'pred':y_pred})\n",
        "results = results.sort_values(by='Accuracy', ascending=False)\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "wdtgR-ERBt6l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}